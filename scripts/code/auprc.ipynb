{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29f39ca-9d0f-4804-b98c-126480297742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (from xgboost) (1.14.0)\n",
      "Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of devscripts: Invalid version: '2.22.1ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b310705b-b184-4cc3-9a33-775401cfb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from transformers import set_seed\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0af6abfe-4469-47d0-87c1-38e072c83e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_df full.csv\")\n",
    "train_data = pd.read_csv(\"new_train_df (1).csv\")\n",
    "test_data.drop(columns = [\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de36879-e946-422a-8d0c-e8d118abac78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>is_in_same_sent</th>\n",
       "      <th>min_sents_window</th>\n",
       "      <th>sentence</th>\n",
       "      <th>in_neighbors</th>\n",
       "      <th>label</th>\n",
       "      <th>gda_score</th>\n",
       "      <th>gda_disparities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1834320</td>\n",
       "      <td>3073</td>\n",
       "      <td>D013661</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Biochemistry and genetics of @DiseaseTgt$ . @D...</td>\n",
       "      <td>0 0 0 4 2|4 0 6|8 8 8 13 13 13 13 8 16 16 13 8...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10694284</td>\n",
       "      <td>3077</td>\n",
       "      <td>D006432</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>@GeneSrc$ @DiseaseTgt$ . @DiseaseTgt$ is a com...</td>\n",
       "      <td>1 1 1 3|8 8 8 8 8 8 12 12 12 8 8 15 16 16 19 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11845294</td>\n",
       "      <td>1080</td>\n",
       "      <td>D003550</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>@DiseaseTgt$ and @GeneSrc$ . @DiseaseTgt$ ( @D...</td>\n",
       "      <td>0 0 0 0 4|11 6 4 6 11 11 11 11 11 15 15 12 11 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12416729</td>\n",
       "      <td>3077</td>\n",
       "      <td>D006432</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>@GeneSrc$ and @GeneSrc$ @DiseaseTgt$ . @Diseas...</td>\n",
       "      <td>0 0 3 0 0 5|11 7 5 7 11 11 11 14 14 11 16 17 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12416729</td>\n",
       "      <td>3077</td>\n",
       "      <td>D008107</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>@GeneSrc$ and @GeneSrc$ hemochromatosis . Here...</td>\n",
       "      <td>0 0 3 0 0 6 12 8 6 8 12 12 12 15 15 12 17 18 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>37366140</td>\n",
       "      <td>9829</td>\n",
       "      <td>D010300</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>@DiseaseTgt$ - genetic cause . PURPOSE OF REVI...</td>\n",
       "      <td>0|2|3 2 3 3 3 5 7 5 5 10 19 14 14 14 10 14 15|...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>37387251</td>\n",
       "      <td>4000</td>\n",
       "      <td>D016884</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>A recurrent homozygous @GeneSrc$ missense vari...</td>\n",
       "      <td>6 6 6 6 6 6 7 8 8 11 11 8 11 15 15 12 15 19 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>37468280</td>\n",
       "      <td>4771</td>\n",
       "      <td>C537392</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Current progress in genomics and targeted ther...</td>\n",
       "      <td>1 1 3 1 3 6 1 9 9 6 9 1 13 26 13 16 13 16 13 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539</th>\n",
       "      <td>37522971</td>\n",
       "      <td>3043</td>\n",
       "      <td>D000755</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Recent national trends in outcomes and economi...</td>\n",
       "      <td>2 2 2 4 2 4 7 4 13 13 13 12 13 2 2 15|22 17 15...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>38027257</td>\n",
       "      <td>3043</td>\n",
       "      <td>C536778</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Genome editing for sickle cell disease : still...</td>\n",
       "      <td>1 8 5 5 5 1 1 8 8 10 8 8 14 14 22 16 14 16 22 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4541 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index   id1      id2  is_in_same_sent  min_sents_window  \\\n",
       "0      1834320  3073  D013661            False                 1   \n",
       "1     10694284  3077  D006432             True                 0   \n",
       "2     11845294  1080  D003550             True                 0   \n",
       "3     12416729  3077  D006432             True                 0   \n",
       "4     12416729  3077  D008107            False                 1   \n",
       "...        ...   ...      ...              ...               ...   \n",
       "4536  37366140  9829  D010300             True                 0   \n",
       "4537  37387251  4000  D016884             True                 0   \n",
       "4538  37468280  4771  C537392             True                 0   \n",
       "4539  37522971  3043  D000755             True                 0   \n",
       "4540  38027257  3043  C536778             True                 0   \n",
       "\n",
       "                                               sentence  \\\n",
       "0     Biochemistry and genetics of @DiseaseTgt$ . @D...   \n",
       "1     @GeneSrc$ @DiseaseTgt$ . @DiseaseTgt$ is a com...   \n",
       "2     @DiseaseTgt$ and @GeneSrc$ . @DiseaseTgt$ ( @D...   \n",
       "3     @GeneSrc$ and @GeneSrc$ @DiseaseTgt$ . @Diseas...   \n",
       "4     @GeneSrc$ and @GeneSrc$ hemochromatosis . Here...   \n",
       "...                                                 ...   \n",
       "4536  @DiseaseTgt$ - genetic cause . PURPOSE OF REVI...   \n",
       "4537  A recurrent homozygous @GeneSrc$ missense vari...   \n",
       "4538  Current progress in genomics and targeted ther...   \n",
       "4539  Recent national trends in outcomes and economi...   \n",
       "4540  Genome editing for sickle cell disease : still...   \n",
       "\n",
       "                                           in_neighbors  label  gda_score  \\\n",
       "0     0 0 0 4 2|4 0 6|8 8 8 13 13 13 13 8 16 16 13 8...      1   0.650000   \n",
       "1     1 1 1 3|8 8 8 8 8 8 12 12 12 8 8 15 16 16 19 1...      1   0.650000   \n",
       "2     0 0 0 0 4|11 6 4 6 11 11 11 11 11 15 15 12 11 ...      1   0.573333   \n",
       "3     0 0 3 0 0 5|11 7 5 7 11 11 11 14 14 11 16 17 1...      0   0.650000   \n",
       "4     0 0 3 0 0 6 12 8 6 8 12 12 12 15 15 12 17 18 1...      0   0.100000   \n",
       "...                                                 ...    ...        ...   \n",
       "4536  0|2|3 2 3 3 3 5 7 5 5 10 19 14 14 14 10 14 15|...      1   0.050000   \n",
       "4537  6 6 6 6 6 6 7 8 8 11 11 8 11 15 15 12 15 19 19...      1   0.060000   \n",
       "4538  1 1 3 1 3 6 1 9 9 6 9 1 13 26 13 16 13 16 13 2...      1   0.200000   \n",
       "4539  2 2 2 4 2 4 7 4 13 13 13 12 13 2 2 15|22 17 15...      1   0.800000   \n",
       "4540  1 8 5 5 5 1 1 8 8 10 8 8 14 14 22 16 14 16 22 ...      1   0.010000   \n",
       "\n",
       "      gda_disparities  \n",
       "0                 2.0  \n",
       "1                 2.0  \n",
       "2                 3.0  \n",
       "3                 2.0  \n",
       "4                 1.0  \n",
       "...               ...  \n",
       "4536              1.0  \n",
       "4537              1.0  \n",
       "4538              1.0  \n",
       "4539              1.0  \n",
       "4540              1.0  \n",
       "\n",
       "[4541 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845f92cf-c8f7-4e9f-890c-e8271de92ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioBERT model loaded\n",
      "Tokenizer loaded\n",
      "Dataset created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 2791/4541 [00:38<00:28, 60.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salidroside Attenuates Cognitive Dysfunction in Senescence-Accelerated Mouse Prone 8 ( SAMP8 ) Mice and Modulates Inflammation of the Gut-Brain Axis . Background : Alzheimer 's disease ( AD ) is a fatal neurodegenerative disease characterized by progressive cognitive decline and memory loss . However , several therapeutic approaches have shown unsatisfactory outcomes in the clinical setting . Thus , developing alternative therapies for the prevention and treatment of AD is critical . Salidroside ( SAL ) is critical , an herb-derived phenylpropanoid glycoside compound , has been shown to attenuate lipopolysaccharide (LPS)-induced cognitive impairment . However , the mechanism underlying its neuroprotective effects remains unclear . Here , we show that SAL has a therapeutic effect in the senescence-accelerated mouse prone 8 ( SAMP8 ) strain , a reliable and stable mouse model of AD . Methods : SAMP8 mice were treated with SAL , donepezil ( DNP ) or saline , and cognitive behavioral impairments were assessed using the Morris water maze ( MWM ) , Y maze , and open field test ( OFT ) . Fecal samples were collected and analyzed by 16S rRNA sequencing on an Illumina MiSeq system . Brain samples were analyzed to detect beta-amyloid ( Abeta ) 1 - 42 ( Abeta1 - 42 ) deposition by immunohistochemistry ( IHC ) and western blotting . The activation of microglia and neuroinflammatory cytokines was detected by immunofluorescence ( IF ) , western blotting and qPCR . Serum was analyzed by a Mouse High Sensitivity T Cell Magnetic Bead Panel on a Luminex-MAGPIX multiplex immunoassay system . Results : Our results suggest that SAL effectively alleviated hippocampus-dependent memory impairment in the SAMP8 mice . SAL significantly 1 ) reduced toxic Abeta1 - 42 deposition ; 2 ) reduced microglial activation and attenuated the levels of the proinflammatory factors IL-1beta , IL-6 , and TNF-alpha in the brain ; 3 ) improved the gut barrier integrity and modified the gut microbiota ( reversed the ratio of Bacteroidetes to Firmicutes and eliminated Clostridiales and Streptococcaceae , which may be associated with cognitive deficits ) ; and 4 ) decreased the levels of proinflammatory cytokines , particularly IL-1alpha , IL-6 , @GeneSrc$ and IL-12 , in the peripheral circulation , as determined by a multiplex immunoassay . Conclusion : In summary , SAL reversed AD-related changes in SAMP8 mice , potentially by regulating the microbiota-gut-brain axis and modulating @DiseaseTgt$ in both the peripheral circulation and central nervous system . Our results strongly suggest that SAL has a preventive effect on cognition-related changes in SAMP8 mice and highlight its value as a potential agent for drug development .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 2906/4541 [00:40<00:26, 61.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Systemic infection exacerbates cerebrovascular dysfunction in Alzheimer 's disease . We studied the effects of systemic infection on brain cytokine level and cerebral vascular function in Alzheimer 's disease and vascular dementia , in superior temporal cortex ( Brodmann area 22 ) from Alzheimer 's disease patients ( n = 75 ) , vascular dementia patients ( n = 22 ) and age-matched control subjects ( n = 46 ) , stratified according to the presence or absence of terminal systemic infection . Brain cytokine levels were measured using Mesoscale Discovery Multiplex Assays and markers of cerebrovascular function were assessed by ELISA . Multiple brain cytokines were elevated in Alzheimer 's disease and vascular dementia : IL-15 and IL-17A were maximally elevated in end-stage Alzheimer 's disease ( Braak tangle stage V-VI ) whereas IL-2 , IL-5 , IL12p40 and IL-16 were highest in intermediate Braak tangle stage III-IV disease . Several cytokines ( IL-1beta , IL-6 , TNF-alpha , IL-8 and IL-15 ) were further raised in Alzheimer 's disease with systemic infection . Cerebral hypoperfusion-indicated by decreased MAG : PLP1 and increased vascular endothelial growth factor-A (VEGF)-and blood-brain barrier leakiness , indicated by raised levels of fibrinogen , were exacerbated in Alzheimer 's disease and vascular dementia patients , and also in non-dementia controls , with systemic infection . Amyloid-beta42 level did not vary with infection or in association with brain cytokine levels . In controls , cortical perfusion declined with increasing IFN-gamma , IL-2 , IL-4 , IL-6 , IL-10 , IL-12p70 , IL-13 and tumour necrosis factor-alpha ( TNF-alpha ) but these relationships were lost with progression of Alzheimer 's disease , and with infection ( even in Braak stage 0-II brains ) . Cortical platelet-derived growth factor receptor-beta ( PDGFRbeta ) , a pericyte marker , was reduced , and endothelin-1 ( EDN1 ) level was increased in Alzheimer 's disease ; these were related to @GeneSrc$ level and disease progression and only modestly affected by systemic infection . Our findings indicate that systemic infection alters brain cytokine levels and exacerbates @DiseaseTgt$ associated with Alzheimer 's disease and vascular dementia , independently of the level of insoluble @GeneSrc$ , and highlight systemic infection as an important contributor to dementia , requiring early identification and treatment in the elderly population .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4541/4541 [01:04<00:00, 70.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:08<00:00, 72.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated\n",
      "Dataset created\n",
      "Embeddings generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class model_trainer:\n",
    "\n",
    "    def __init__(self, train_data, test_data, model_name):\n",
    "        self.train_df= train_data\n",
    "        self.test_df= test_data\n",
    "        self.model_name=self.load_model(model_name)\n",
    "        self.tokenizer=self.load_tokenizer(model_name)\n",
    "#         self.model_name.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "\n",
    "    def load_model(self,model_name):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        print(\"BioBERT model loaded\")\n",
    "        return biobert\n",
    "\n",
    "    def load_tokenizer(self,model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#         tokenizer.add_tokens([\"GeneSrc\", \"DiseaseTgt\", \"causative\", \"causal\", \"cause\", \"causing\", \"caused\"])\n",
    "        print(\"Tokenizer loaded\")\n",
    "        return tokenizer\n",
    "\n",
    "    def load_dataset(self,data_path):\n",
    "        df = pd.read_csv(data_path)\n",
    "        return df\n",
    "    \n",
    "    def remove_stopwords(self,text):\n",
    "        #nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    \n",
    "    def create_dataset(self):\n",
    "        # Apply the remove_stopwords function to the 'sentence' column\n",
    "        # self.train_df['sentence'] = self.train_df['sentence'].apply(self.remove_stopwords)\n",
    "        # self.test_df['sentence'] = self.test_df['sentence'].apply(self.remove_stopwords)\n",
    "\n",
    "        X_train = self.train_df['sentence'].tolist()\n",
    "        y_train = self.train_df['label'].tolist()\n",
    "\n",
    "        X_test = self.test_df['sentence'].tolist()\n",
    "        y_test = self.test_df['label'].tolist()\n",
    "        \n",
    "        print(\"Dataset created\")\n",
    "        # print(X_test[0])\n",
    "        return X_train,y_train,X_test,y_test\n",
    "    \n",
    "    def get_specific_token_embeddings(self,sentence):\n",
    "        # 1. Tokenize the input sentence\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "\n",
    "        # 2. Find the indices of \"@GeneSrc\" and \"@DiseaseTgt$\"\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        gene_src_token = self.tokenizer.tokenize(\"@GeneSrc$\")\n",
    "        disease_tgt_token = self.tokenizer.tokenize(\"@DiseaseTgt$\")\n",
    "\n",
    "        gene_src_indices = [i for i, token in enumerate(tokenized_sentence) if token in gene_src_token]\n",
    "        disease_tgt_indices = [i for i, token in enumerate(tokenized_sentence) if token in disease_tgt_token]\n",
    "\n",
    "        # Run the sentence through BioBERT\n",
    "        with torch.no_grad():\n",
    "            #print(self.model_name)\n",
    "            outputs = self.model_name(**inputs)\n",
    "        embeddings = outputs['last_hidden_state'][0]  # Extracting embeddings for the whole sentence\n",
    "\n",
    "        # 3. Retrieve the embeddings for the surrounding tokens\n",
    "        context_range = 2\n",
    "\n",
    "        def get_context_embeddings(indices):\n",
    "            context_embeddings = []\n",
    "            for idx in indices:\n",
    "                start = max(0, idx - context_range)\n",
    "                end = min(idx + context_range + 1, len(tokenized_sentence))\n",
    "                context = embeddings[start:end]\n",
    "                context_embeddings.append(context)\n",
    "            return torch.cat(context_embeddings).view(-1, 768)\n",
    "\n",
    "        gene_src_embeddings = get_context_embeddings(gene_src_indices)\n",
    "        disease_tgt_embeddings = get_context_embeddings(disease_tgt_indices)\n",
    "\n",
    "        # 4. Compute the average of the embeddings\n",
    "        avg_gene_src_embedding = torch.mean(gene_src_embeddings, dim=0)\n",
    "        avg_disease_tgt_embedding = torch.mean(disease_tgt_embeddings, dim=0)\n",
    "\n",
    "        combined_embedding = torch.cat([avg_gene_src_embedding, avg_disease_tgt_embedding], dim=0)\n",
    "        combined_embedding_np = combined_embedding.cpu().numpy().reshape(1, -1)  # Convert tensor to NumPy array and reshape to 2D\n",
    "\n",
    "        if np.isnan(combined_embedding_np).any():\n",
    "            print(sentence)\n",
    "\n",
    "        return combined_embedding_np\n",
    "\n",
    "        \n",
    "        \n",
    "    # def generate_embeddings(self):\n",
    "    #     X_train,y_train,X_test,y_test=self.create_dataset()\n",
    "    #     try:\n",
    "    #         X_train_embeddings = np.vstack([self.get_specific_token_embeddings(sentence) for sentence in tqdm(X_train)])\n",
    "    #         X_test_embeddings = np.vstack([self.get_specific_token_embeddings(sentence) for sentence in tqdm(X_test)])\n",
    "           \n",
    "    #     except Exception as e:\n",
    "    #         print(e)\n",
    "    #         pass\n",
    "    #     print(\"Embeddings generated\")\n",
    "    #     return X_train_embeddings,X_test_embeddings\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        X_train, y_train, X_test, y_test = self.create_dataset()\n",
    "        \n",
    "        X_train_embeddings = []\n",
    "        valid_train_indices = []\n",
    "        X_test_embeddings = []\n",
    "        valid_test_indices = []\n",
    "        \n",
    "        # Generate train embeddings\n",
    "        for idx, sentence in enumerate(tqdm(X_train)):\n",
    "            embedding = self.get_specific_token_embeddings(sentence)\n",
    "            if embedding is not None:\n",
    "                X_train_embeddings.append(embedding)\n",
    "                valid_train_indices.append(idx)\n",
    "        \n",
    "        # Filter labels based on valid embeddings\n",
    "        y_train = [y_train[idx] for idx in valid_train_indices]\n",
    "        \n",
    "        # Generate test embeddings\n",
    "        for idx, sentence in enumerate(tqdm(X_test)):\n",
    "            embedding = self.get_specific_token_embeddings(sentence)\n",
    "            if embedding is not None:\n",
    "                X_test_embeddings.append(embedding)\n",
    "                valid_test_indices.append(idx)\n",
    "        \n",
    "        # Filter labels based on valid embeddings\n",
    "        y_test = [y_test[idx] for idx in valid_test_indices]\n",
    "        \n",
    "        X_train_embeddings = np.vstack(X_train_embeddings)\n",
    "        X_test_embeddings = np.vstack(X_test_embeddings)\n",
    "        \n",
    "        print(\"Embeddings generated\")\n",
    "        return X_train_embeddings, X_test_embeddings\n",
    "\n",
    "    \n",
    "    def drop_null_embeddings(self,X_train_embeddings,X_test_embeddings,y_train,y_test):\n",
    "        X_train_embeddings=pd.DataFrame(X_train_embeddings).dropna()\n",
    "        X_test_embeddings=pd.DataFrame(X_test_embeddings).dropna()\n",
    "\n",
    "        test_ind=[i for i in range(0,len(X_test_embeddings)) if i not in X_test_embeddings.index]\n",
    "        train_ind=[i for i in range(0,len(X_train_embeddings)) if i not in X_train_embeddings.index]\n",
    "        for i in train_ind:\n",
    "            y_train.pop(i)\n",
    "        for i in test_ind:\n",
    "            y_test.pop(i)\n",
    "            \n",
    "        return X_train_embeddings,X_test_embeddings,y_train,y_test\n",
    "    \n",
    "    def svm_classifiation(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using SVM...\")\n",
    "        clf = SVC(kernel='poly', degree=20, probability=True, class_weight={0:1, 1:30})\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of SVM classifier are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        probas = clf.predict_proba(X_test_embeddings)  # Get probabilities for the positive class\n",
    "        y_scores = probas[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "        # Example: Compute precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "        \n",
    "        # Example: Compute AUC (Area Under Curve) for precision-recall curve\n",
    "        auprc = auc(recall, precision)\n",
    "        \n",
    "        print(f'Area Under Precision-Recall Curve (AUPRC): {auprc:.2f}')\n",
    "        \n",
    "        \n",
    "    def xg_boost_classification(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using XG Boost...\")\n",
    "        \n",
    "        clf = xgb.XGBClassifier(scale_pos_weight=200, max_depth=40, learning_rate=0.2, n_estimators=100, gamma=0.3)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        probas = clf.predict_proba(X_test_embeddings)  # Get probabilities for the positive class\n",
    "        y_scores = probas[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "        # Example: Compute precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "        \n",
    "        # Example: Compute AUC (Area Under Curve) for precision-recall curve\n",
    "        auprc = auc(recall, precision)\n",
    "        \n",
    "        print(f'Area Under Precision-Recall Curve (AUPRC): {auprc:.2f}')\n",
    "\n",
    "        print(\"Results of XG Boost are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    def random_forest(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using Random Forest...\")\n",
    "        \n",
    "\n",
    "        # Instantiate the Random Forest Classifier\n",
    "        clf = RandomForestClassifier(n_estimators=100, max_depth=10, max_leaf_nodes= 500, n_jobs= 2, max_features=\"sqrt\", class_weight={0:1, 1:30}, verbose=True)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of Random Forest are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        probas = clf.predict_proba(X_test_embeddings)  # Get probabilities for the positive class\n",
    "        y_scores = probas[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "        # Example: Compute precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "        \n",
    "        # Example: Compute AUC (Area Under Curve) for precision-recall curve\n",
    "        auprc = auc(recall, precision)\n",
    "        \n",
    "        print(f'Area Under Precision-Recall Curve (AUPRC): {auprc:.2f}')\n",
    "        \n",
    "    def cat_boost(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using Cat Boost...\")\n",
    "        \n",
    "        clf = CatBoostClassifier(iterations=500, \n",
    "                         depth=10, \n",
    "                         learning_rate=0.1, \n",
    "                         custom_loss='Logloss',\n",
    "                         verbose=200,\n",
    "                         class_weights=[1, 30])\n",
    "\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of Cat Boost classifier are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    def interpretation_call(self):\n",
    "\n",
    "        causal_test_df=self.test_df[self.test_df[\"label\"]==1]\n",
    "        causal_df_unique = causal_test_df.drop_duplicates(subset=['index', 'id1', 'id2'])\n",
    "\n",
    "        samp_abst=causal_df_unique[causal_df_unique[\"index\"]==25064704]\n",
    "\n",
    "        # Load the model from a file\n",
    "        clf = load('CRED_trained_model.joblib')\n",
    "\n",
    "        model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        for _, row in tqdm(causal_df_unique.iterrows()):\n",
    "            ranked, word_importance = interpretation(row, clf, tokenizer, biobert, self.get_specific_token_embeddings)\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "mod_tr = model_trainer(train_data, test_data, \"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "#To generate embeddings uncomment the below line\n",
    "X_train_embeddings,X_test_embeddings= mod_tr.generate_embeddings()\n",
    "\n",
    "\n",
    "X_train,y_train,X_test,y_test = mod_tr.create_dataset()\n",
    "\n",
    "X_train_embeddings,X_test_embeddings,y_train,y_test = mod_tr.drop_null_embeddings(X_train_embeddings,X_test_embeddings,y_train,y_test)\n",
    "print(\"Embeddings generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2274cfc-5e64-4de7-89ce-38d7738b516d",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88c7eb7-e113-4e46-961e-f21ad5296573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def svm_classification(X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "    print(\"Doing classification using SVM...\")\n",
    "    clf = SVC(kernel='poly', degree=10, probability=True, class_weight={0:1, 1:30}, gamma='scale', C=10)\n",
    "    clf.fit(X_train_embeddings, y_train)\n",
    "    dump(clf, 'svm_model.joblib')\n",
    "    print('model saved')\n",
    "\n",
    "\n",
    "    probas = clf.predict_proba(X_test_embeddings)  # Get probabilities for the positive class\n",
    "    y_scores = probas[:, 1]  # Probabilities for class 1\n",
    "\n",
    "    # Example: Compute precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "    \n",
    "    # Example: Compute AUC (Area Under Curve) for precision-recall curve\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    print(f'Area Under Precision-Recall Curve (AUPRC): {auprc:.2f}')\n",
    "\n",
    "    return probas, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4d7e9a-481a-4587-b935-2d765c9a5b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification using SVM...\n",
      "model saved\n",
      "Area Under Precision-Recall Curve (AUPRC): 0.47\n"
     ]
    }
   ],
   "source": [
    "proba, clf = svm_classification(X_train_embeddings,y_train,X_test_embeddings,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92159487-ca3d-4d43-8458-42824792380d",
   "metadata": {},
   "source": [
    "# XG-boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5764dc3a-d254-4651-ae2d-659347f093de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_boost_classification( X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "    print(\"Doing classification using XG Boost...\")\n",
    "        \n",
    "    clf = xgb.XGBClassifier(scale_pos_weight=500, max_depth=40, learning_rate=0.1, n_estimators=400, gamma=0.1)\n",
    "    clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "    # Get probabilities\n",
    "    probas = clf.predict_proba(X_test_embeddings)\n",
    "    y_scores = probas[:, 1]  # Probabilities for class 1\n",
    "\n",
    "    # Example: Compute precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "    \n",
    "    # Example: Compute AUC (Area Under Curve) for precision-recall curve\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    print(f'Area Under Precision-Recall Curve (AUPRC): {auprc:.2f}')\n",
    "\n",
    "    return probas, clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6de71a6a-6de6-4a1c-941a-9b42263ef90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification using XG Boost...\n",
      "Area Under Precision-Recall Curve (AUPRC): 0.38\n"
     ]
    }
   ],
   "source": [
    "proba, clf = xg_boost_classification(X_train_embeddings,y_train,X_test_embeddings,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c99fb-9f45-4bf3-8a02-771d3f78db40",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c278c87-9cb8-42fb-acab-0e3d91444437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_forest(X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "\n",
    "\n",
    "\n",
    "    # Instantiate the Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=200,n_jobs=5, max_depth=50, max_leaf_nodes= 500,  max_features=\"sqrt\", class_weight={0:1, 1:30})\n",
    "    clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "    probas = clf.predict_proba(X_test_embeddings)\n",
    "    y_scores = probas[:, 1]  # Probabilities for class 1\n",
    "\n",
    "    # Example: Compute precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "    \n",
    "    # Example: Compute AUC (Area Under Curve) for precision-recall curve\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    print(f'Area Under Precision-Recall Curve (AUPRC): {auprc:.2f}')\n",
    "\n",
    "    return probas, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d05351e9-be57-4b98-b7c8-b5e54f25d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Precision-Recall Curve (AUPRC): 0.41\n"
     ]
    }
   ],
   "source": [
    "proba, clf = random_forest(X_train_embeddings,y_train,X_test_embeddings,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c058e7-fcb8-4d8a-afb9-16de8a395bb9",
   "metadata": {},
   "source": [
    "# Pubmedbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71842377-7544-49c2-8753-61055fb070d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Precision-Recall Curve (AUPRC): 0.42\n"
     ]
    }
   ],
   "source": [
    "pubmed_bert = pd.read_csv(\"pubmedbert_probs_new_data.csv\")\n",
    "# Calculate the average precision score\n",
    "y_scores_pub_bert = pubmed_bert[\"1\"]\n",
    "precision_pub_bert, recall_pub_bert, _ = precision_recall_curve(y_test, y_scores_pub_bert)\n",
    "auprc_pub_bert = auc(recall_pub_bert, precision_pub_bert)\n",
    "\n",
    "print(f'Area Under Precision-Recall Curve (AUPRC): {auprc_pub_bert:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93271bf8-531d-4805-92f4-2cfaa8fc6b6d",
   "metadata": {},
   "source": [
    "# Scibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8244e80-e85a-4048-a1fb-75882bd7210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under Precision-Recall Curve (AUPRC): 0.46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "scibert = pd.read_csv(\"scibert_probs_new_data.csv\")\n",
    "\n",
    "\n",
    "# Calculate the average precision score\n",
    "y_scores_scibert = scibert[\"1\"]\n",
    "precision_scibert, recall_scibert, _ = precision_recall_curve(y_test, y_scores_scibert)\n",
    "auprc_scibert = auc(recall_scibert, precision_scibert)\n",
    "\n",
    "print(f'Area Under Precision-Recall Curve (AUPRC): {auprc_scibert:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354da9c-a469-4e43-88d5-cfdb273384cf",
   "metadata": {},
   "source": [
    "# Biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed2010c6-9a8b-4ec2-ac91-4206c953d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55e3b6625364a6f85c7f562cdc2f9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2444c6b18b45658877d0aa3f0b0f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2272' max='2272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2272/2272 04:25, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.334235</td>\n",
       "      <td>0.923577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.326716</td>\n",
       "      <td>0.913821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.176300</td>\n",
       "      <td>0.448266</td>\n",
       "      <td>0.921951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.469814</td>\n",
       "      <td>0.928455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9495652173913044, 'recall': 0.9732620320855615, 'f1-score': 0.9612676056338029, 'support': 561.0}, '1': {'precision': 0.625, 'recall': 0.46296296296296297, 'f1-score': 0.5319148936170213, 'support': 54.0}, 'accuracy': 0.9284552845528455, 'macro avg': {'precision': 0.7872826086956521, 'recall': 0.7181124975242622, 'f1-score': 0.7465912496254121, 'support': 615.0}, 'weighted avg': {'precision': 0.9210668080593849, 'recall': 0.9284552845528455, 'f1-score': 0.9235683431152562, 'support': 615.0}}\n",
      "Area Under Precision-Recall Curve (AUPRC): 0.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "# Define the compute metrics function for evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': (preds == labels).mean()}\n",
    "\n",
    "# Load datasets\n",
    "test_df = pd.read_csv(\"test_df full.csv\")\n",
    "train_df = pd.read_csv(\"new_train_df (1).csv\")\n",
    "test_df.drop(columns = [\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "train_df = train_df[[\"sentence\", \"label\"]]\n",
    "test_df = test_df[[\"sentence\", \"label\"]]\n",
    "\n",
    "\n",
    "# Convert your dataframes to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "# Tokenize your dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# Set format for pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bioresults',            # output directory\n",
    "    num_train_epochs=4,                   # total number of training epochs\n",
    "    learning_rate=3e-5,                   # learning rate\n",
    "    per_device_train_batch_size=8,        # batch size per device during training\n",
    "    per_device_eval_batch_size=4,         # batch size for evaluation\n",
    "    warmup_steps=1000,                       # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.05,                    # strength of weight decay\n",
    "    logging_dir='./biologs',              # directory for storing logs\n",
    "    load_best_model_at_end=True,          # load the best model at the end of training\n",
    "    metric_for_best_model=\"accuracy\",     # use accuracy for best model\n",
    "    evaluation_strategy=\"epoch\",          # evaluate each epoch\n",
    "    save_strategy=\"epoch\",                # save checkpoints each epoch\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=2)\n",
    "\n",
    "# Save the model with tensors made contiguous\n",
    "def save_model_with_contiguous(model, output_dir):\n",
    "    # Ensure all tensors are contiguous before saving\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "# Modify the Trainer to include the new saving function\n",
    "class CustomTrainer(Trainer):\n",
    "    def _save_checkpoint(self, model, trial, metrics=None):\n",
    "        output_dir = self._get_output_dir(trial=trial)\n",
    "        self.save_model(output_dir)\n",
    "\n",
    "    def save_model(self, output_dir, _internal_call=True):\n",
    "        save_model_with_contiguous(self.model, output_dir)\n",
    "\n",
    "# Use CustomTrainer instead of Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # metrics function for evaluation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "probabilities = softmax(predictions.predictions, axis=1)\n",
    "\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n",
    "\n",
    "# Calculate the average precision score\n",
    "y_scores_bluebert = probabilities[:, 1]\n",
    "precision_bluebert, recall_bluebert, _ = precision_recall_curve(y_test, y_scores_bluebert)\n",
    "auprc_bluebert = auc(recall_bluebert, precision_bluebert)\n",
    "\n",
    "print(f'Area Under Precision-Recall Curve (AUPRC): {auprc_bluebert:.2f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cd460-83f5-48ea-9da6-027a444065b1",
   "metadata": {},
   "source": [
    "SVM: 0.47 <br>\n",
    "XG Boost: 0.38 <br>\n",
    "Random Forest: 0.41 <br>\n",
    "Pubmedbert: 0.42 <br>\n",
    "Scibert: 0.46 <br>\n",
    "Biobert: 0.43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52230edd-e292-4996-a442-a98746b98e99",
   "metadata": {},
   "source": [
    "# Ends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
