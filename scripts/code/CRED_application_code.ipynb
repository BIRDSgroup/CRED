{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548a3151-68b2-47c2-bb38-956d941d2dc9",
   "metadata": {},
   "source": [
    "# Loading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b319e-ae8e-4550-85c5-5cc3c377697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SPARQLWrapper torch transformers spacy pandas numpy matplotlib seaborn requests tqdm nltk transformers scikit-learn pickle5 lxml beautifulsoup4 wandb igraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993740c3-64cf-4862-81a9-2a25ea6a7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import igraph as ig\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150799e-3de1-4eea-9ac3-bd7119764b64",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d8c5d-7acb-42be-a72a-c88d61516447",
   "metadata": {},
   "source": [
    "The text files contains PubTator format PubMed articles, including gene-disease entity pairs and other related information. A Python script processes this file to generate BERT format in TSV format. To enhance efficiency, the original text file has been split into 5 separate text files, each with its own Python script, resulting in 5 corresponding TSV files. <br>\n",
    "Python files to convert txt files into tsv - part1.py, part2.py, part3.py, part4.py, part5.py <br>\n",
    "Run these python files to get tsv files. Or you can directly skip one cell (the next code cell) to get the pre-processed data loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7437d074-95f0-423f-a257-9031b5aa0e3d",
   "metadata": {},
   "source": [
    "Preparing dataset for Parkinsons disease application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b138d-13d3-44c1-b0ae-b76b07408c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('part1.tsv', sep='\\t')\n",
    "df2 = pd.read_csv('part2.tsv', sep='\\t')\n",
    "df3 = pd.read_csv('part3.tsv', sep='\\t')\n",
    "df4 = pd.read_csv('part4.tsv', sep='\\t')\n",
    "df5 = pd.read_csv('part5.tsv', sep='\\t')\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5], axis=0, ignore_index=True)\n",
    "df = df.drop_duplicates(subset = ['index','id1','id2','sentence'])\n",
    "df_filtered = df[df['id2']=='D010300']\n",
    "\n",
    "def get_gene_symbol(ncbi_id):\n",
    "    if \";\" in str(ncbi_id):\n",
    "        ncbi_id = str(ncbi_id).split(\";\")[0]\n",
    "\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"gene\",\n",
    "        \"id\": ncbi_id,\n",
    "        \"retmode\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    gene_symbol = data[\"result\"][str(ncbi_id)][\"name\"]\n",
    "    return gene_symbol\n",
    "\n",
    "\n",
    "def create_ncbi_to_gene_symbol_dict(ncbi_ids):\n",
    "    ncbi_to_gene_symbol = {}\n",
    "    error_ids = []\n",
    "    request_count = 0\n",
    "    \n",
    "    for ncbi_id in tqdm(ncbi_ids, desc=\"Fetching gene symbols\"):\n",
    "        if request_count > 0 and request_count % 3 == 0:\n",
    "            time.sleep(1)  # Sleep for 1 second after every 3 requests\n",
    "\n",
    "        try:\n",
    "            gene_symbol = get_gene_symbol(ncbi_id)\n",
    "            ncbi_to_gene_symbol[ncbi_id] = gene_symbol\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching gene symbol for NCBI ID {ncbi_id}: {e}\")\n",
    "            error_ids.append(ncbi_id)\n",
    "        \n",
    "        request_count += 1\n",
    "    \n",
    "    # Retry for error IDs\n",
    "    for ncbi_id in tqdm(error_ids, desc=\"Retrying for error IDs\"):\n",
    "        if request_count > 0 and request_count % 3 == 0:\n",
    "            time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            gene_symbol = get_gene_symbol(ncbi_id)\n",
    "            ncbi_to_gene_symbol[ncbi_id] = gene_symbol\n",
    "            error_ids.remove(ncbi_id)  # Remove from error list if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching gene symbol for NCBI ID {ncbi_id}: {e}\")\n",
    "        \n",
    "        request_count += 1\n",
    "    \n",
    "    return ncbi_to_gene_symbol, error_ids\n",
    "\n",
    "\n",
    "ncbi_to_gene_symbol_dict, remaining_errors = create_ncbi_to_gene_symbol_dict(genes)\n",
    "\n",
    "df_filtered['Symbol'] = df_filtered['id1'].map(ncbi_to_gene_symbol_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bf70b-31f7-431a-a121-b368c33affa3",
   "metadata": {},
   "source": [
    "Run from here to load the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ba90b-4dde-4f9d-8e2d-f0395db0e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered=pd.read_csv('park_filtered.csv')\n",
    "\n",
    "park = df[df['id2']=='D010300']\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, df, model_name):\n",
    "        self.df = df\n",
    "        self.model_name = self.load_model(model_name)\n",
    "        self.tokenizer = self.load_tokenizer(model_name)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        print(\"BioBERT model loaded\")\n",
    "        return biobert\n",
    "\n",
    "    def load_tokenizer(self, model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        print(\"Tokenizer loaded\")\n",
    "        return tokenizer\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # self.df['sentence'] = self.df['sentence'].apply(self.remove_stopwords)\n",
    "        X = self.df['sentence'].tolist()\n",
    "        print(\"Dataset created\")\n",
    "        return X\n",
    "\n",
    "    def get_specific_token_embeddings(self, sentence):\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        gene_src_token = self.tokenizer.tokenize(\"@GeneSrc$\")\n",
    "        disease_tgt_token = self.tokenizer.tokenize(\"@DiseaseTgt$\")\n",
    "\n",
    "        gene_src_indices = [i for i, token in enumerate(tokenized_sentence) if token in gene_src_token]\n",
    "        disease_tgt_indices = [i for i, token in enumerate(tokenized_sentence) if token in disease_tgt_token]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_name(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "        context_range = 2\n",
    "\n",
    "        def get_context_embeddings(indices):\n",
    "            context_embeddings = []\n",
    "            for idx in indices:\n",
    "                start = max(0, idx - context_range)\n",
    "                end = min(idx + context_range + 1, len(tokenized_sentence))\n",
    "                context = embeddings[start:end]\n",
    "                context_embeddings.append(context)\n",
    "            return torch.cat(context_embeddings).view(-1, 768)\n",
    "\n",
    "        gene_src_embeddings = get_context_embeddings(gene_src_indices)\n",
    "        disease_tgt_embeddings = get_context_embeddings(disease_tgt_indices)\n",
    "\n",
    "        avg_gene_src_embedding = torch.mean(gene_src_embeddings, dim=0)\n",
    "        avg_disease_tgt_embedding = torch.mean(disease_tgt_embeddings, dim=0)\n",
    "\n",
    "        combined_embedding = torch.cat([avg_gene_src_embedding, avg_disease_tgt_embedding], dim=0)\n",
    "        combined_embedding_np = combined_embedding.cpu().numpy().reshape(1, -1)\n",
    "\n",
    "        if np.isnan(combined_embedding_np).any():\n",
    "            print(sentence)\n",
    "\n",
    "        return combined_embedding_np\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        X = self.create_dataset()\n",
    "        try:\n",
    "            X_embeddings = []\n",
    "            for sentence in tqdm(X):\n",
    "                sentence_embedding = self.get_specific_token_embeddings(sentence)\n",
    "                X_embeddings.append(sentence_embedding)\n",
    "            X_embeddings = np.vstack(X_embeddings)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        print(\"Embeddings generated\")\n",
    "        return X_embeddings\n",
    "\n",
    "    def svm_classification(self, embeddings):\n",
    "        print(\"Doing classification using SVM...\")\n",
    "        \n",
    "        with open('./model_non_gda.pkl', 'rb') as f:\n",
    "            non_gda_model = pickle.load(f)\n",
    "        \n",
    "        # Initialize lists to store predictions and probabilities\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        error_idx = []\n",
    "        for idx, embedding in enumerate(embeddings):\n",
    "            try:\n",
    "                # Prediction probability for the current embedding\n",
    "                proba = non_gda_model.predict_proba(embedding.reshape(1, -1))\n",
    "                probabilities.append(proba[:, 1][0])\n",
    "                # Prediction for the current embedding\n",
    "                y_pred = non_gda_model.predict(embedding.reshape(1, -1))\n",
    "                predictions.append(y_pred[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Error at index {idx}: {e}\")\n",
    "                error_idx.append(idx)\n",
    "                # Append NaN or any placeholder value to keep the lists aligned with the dataset\n",
    "                probabilities.append(np.nan)\n",
    "                predictions.append(np.nan)\n",
    "        # Add the prediction probabilities and predictions to the dataframe\n",
    "        self.df['non_gda_test_proba'] = probabilities\n",
    "        self.df['Prediction'] = predictions\n",
    "        \n",
    "        # print(len(self.df['Prediction'].isna()))\n",
    "        self.df.to_csv('park_results.csv', index=False)\n",
    "        return error_idx\n",
    "\n",
    "df = df_filtered\n",
    "model_trainer = ModelTrainer(df, 'dmis-lab/biobert-base-cased-v1.1')\n",
    "embeddings = model_trainer.generate_embeddings()\n",
    "error_idx = model_trainer.svm_classification(embeddings)\n",
    "\n",
    "park_results = pd.read_csv('park_results.csv')\n",
    "df_filtered = df_filtered.drop(error_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320e562-77d6-4960-8287-4664b3050472",
   "metadata": {},
   "source": [
    "Fig. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc47b8-f6f5-4b2e-a922-facd42a9e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot of Causal Papers vs. Number of Papers for Each Gene\n",
    "# Group by 'id1' which refers to genes\n",
    "grouped = df_filtered.groupby('id1')\n",
    "\n",
    "# Calculate the number of causal papers (Prediction == 1) for each gene\n",
    "causal = grouped.apply(lambda x: x['Prediction'].sum())\n",
    "\n",
    "# Count the number of unique papers for each gene\n",
    "num_papers = grouped['index'].nunique()\n",
    "\n",
    "# Create a DataFrame for easier filtering\n",
    "stats_df = pd.DataFrame({\n",
    "    'num_papers': num_papers,\n",
    "    'causal': causal,\n",
    "    'Symbol': grouped['Symbol'].first()\n",
    "})\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(stats_df['num_papers'], stats_df['causal'])\n",
    "\n",
    "# Add labels to each point where 'causal' is greater than or equal to 50\n",
    "for i, row in stats_df.iterrows():\n",
    "    if row['causal'] >= 10:\n",
    "        plt.text(row['num_papers'], row['causal'], row['Symbol'], fontsize=11, ha='right')\n",
    "\n",
    "# Set x-axis to log scale\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.xlabel('# Paper with gene mention', fontsize=19)\n",
    "# plt.xlabel('Number of Papers')\n",
    "plt.ylabel('# Causal Prediction', fontsize=19)\n",
    "# plt.title('Scatter Plot of Causal Papers vs. Number of Papers for Each Gene')\n",
    "plt.grid(True)\n",
    "plt.savefig('log zoomed out.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7a254-b2e0-4635-88de-6e6eb695c9a0",
   "metadata": {},
   "source": [
    "For CRED dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989cce7d-4a84-409c-a50f-708d28367d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train hindex.csv')\n",
    "val = pd.read_csv('val hindex.csv')\n",
    "test = pd.read_csv('test hindex.csv')\n",
    "train = train.rename(columns={'H-index':'H-Index'})\n",
    "train.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "val.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "test.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "df = pd.concat([train,test,val], axis=0)\n",
    "df.drop_duplicates(subset=['PMID','id1','id2','sentence'],inplace=Tre)\n",
    "df = df.dropna(subset=['H-Index'])\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Now you can import and use stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Example usage\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Usage outside the class\n",
    "def predict_and_update_df(df, model_path, embeddings):\n",
    "    # Load pre-trained model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Pre-trained model loaded\")\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(embeddings)\n",
    "    \n",
    "    # Add predictions to DataFrame\n",
    "    df['prediction'] = predictions\n",
    "    print(\"Predictions added to DataFrame\")\n",
    "    return df\n",
    "\n",
    "model_path = \"model_non_gda.pkl\"\n",
    "df = predict_and_update_df(df, model_path, X_embeddings)\n",
    "\n",
    "special_pmid = df[df['PMID']==15717024]\n",
    "# special_pmid.to_csv('special_pmid', index=False)\n",
    "\n",
    "gd_grouped = special_pmid.groupby(['id1', 'id2'])['PMID'].count().reset_index()\n",
    "gd_grouped = gd_grouped.rename(columns={'PMID': 'gd abstract count'}) \n",
    "# Count number of pairs where label == 1\n",
    "causal_counts = special_pmid.groupby(['id1', 'id2'])['label'].sum().reset_index()\n",
    "causal_counts = causal_counts.rename(columns={'label': 'Causal_annotations'})\n",
    "\n",
    "# Merge\n",
    "gd_grouped = pd.merge(gd_grouped, causal_counts, on=['id1', 'id2'], how='left')\n",
    "\n",
    "gd_grouped['Causal/Total'] = gd_grouped['Causal_annotations'] / gd_grouped['gd abstract count']\n",
    "# print(gd_grouped)\n",
    "\n",
    "pred_counts = special_pmid.groupby(['id1', 'id2'])['prediction'].sum().reset_index()\n",
    "pred_counts = pred_counts.rename(columns={'prediction': 'Prediction_counts'})\n",
    "\n",
    "# Merge\n",
    "gd_grouped = pd.merge(gd_grouped, pred_counts, on=['id1', 'id2'], how='left')\n",
    "\n",
    "gene_grouped = df.groupby('id1')['PMID'].count().reset_index()\n",
    "gene_grouped.rename(columns={'PMID': 'gene abstract count'}, inplace=True)\n",
    "\n",
    "dis_grouped = df.groupby('id2')['PMID'].count().reset_index()\n",
    "dis_grouped.rename(columns={'PMID': 'disease abstract count'}, inplace=True)\n",
    "\n",
    "special_pmid = pd.merge(special_pmid, gene_grouped, on='id1', how='left')\n",
    "special_pmid = pd.merge(special_pmid, dis_grouped, on='id2', how='left')\n",
    "special_pmid = pd.merge(special_pmid, gd_grouped, on=['id1', 'id2'], how='left')\n",
    "\n",
    "# special_pmid.to_csv('special_pmid_gd_grouped.csv', index=False)\n",
    "special_pmid.to_csv('special_pmid.csv', index=False)\n",
    "print(special_pmid)\n",
    "\n",
    "gd_grouped = pd.merge(gd_grouped, gene_grouped, on='id1', how='left')\n",
    "gd_grouped = pd.merge(gd_grouped, dis_grouped, on='id2', how='left')\n",
    "gene_grouped = df.groupby('id1')['PMID'].count().reset_index()\n",
    "gene_grouped.rename(columns={'PMID': 'gene abstract count'}, inplace=True)\n",
    "gd_grouped = df.groupby(['id1', 'id2'])['PMID'].count().reset_index()\n",
    "gd_grouped = gd_grouped.rename(columns={'PMID': 'gd abstract count'}) \n",
    "\n",
    "causal_counts = df.groupby(['id1', 'id2'])['label'].sum().reset_index()\n",
    "causal_counts = causal_counts.rename(columns={'label': 'Causal_annotations'})\n",
    "\n",
    "# Merge\n",
    "gd_grouped = pd.merge(gd_grouped, causal_counts, on=['id1', 'id2'], how='left')\n",
    "\n",
    "gd_grouped['Causal/Total'] = gd_grouped['Causal_annotations'] / gd_grouped['gd abstract count']\n",
    "\n",
    "df = pd.merge(df, gene_grouped, on='id1', how='left')\n",
    "df = pd.merge(df, dis_grouped, on='id2', how='left')\n",
    "df = pd.merge(df, gd_grouped, on=['id1', 'id2'], how='left')\n",
    "\n",
    "prediction_counts = df.groupby(['id1', 'id2'])['prediction'].sum().reset_index()\n",
    "prediction_counts = prediction_counts.rename(columns={'prediction': 'Prediction_counts'})\n",
    "gd_grouped = pd.merge(gd_grouped, prediction_counts, on=['id1', 'id2'], how='left')\n",
    "\n",
    "gd_grouped = pd.merge(gd_grouped, gene_grouped, on='id1', how='left')\n",
    "\n",
    "# Left merge dis_grouped into the updated gd_grouped on 'id2'\n",
    "gd_grouped = pd.merge(gd_grouped, dis_grouped, on='id2', how='left')\n",
    "gd_grouped = gd_grouped.rename(columns={'id1': 'Gene', 'id2': 'Disease'})\n",
    "# Save the DataFrame to a CSV file\n",
    "gd_grouped.to_csv('gene_disease_grouped.csv', index=False)\n",
    "df = pd.read_csv('gene_disease_grouped.csv')\n",
    "causal = df['Causal/Total'].value_counts()\n",
    "causal.sort_index(ascending=False)\n",
    "\n",
    "# Your email address to be included in Entrez API requests\n",
    "email = \"vanshkapoor9911@gmail.com\"\n",
    "\n",
    "def get_gene_symbol(ncbi_gene_id):\n",
    "    # Define the Entrez URL\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=gene&id={ncbi_gene_id}&retmode=xml&email={email}\"\n",
    "    \n",
    "    # Make the request to Entrez\n",
    "    response = requests.get(url)\n",
    "    # Parse the XML response\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    gene_symbol = None\n",
    "    # Extract the gene symbol from the XML\n",
    "    for docsum in root.findall(\".//DocumentSummary\"):\n",
    "        gene_symbol = docsum.findtext(\"Name\")\n",
    "        break\n",
    "    \n",
    "    return gene_symbol\n",
    "\n",
    "# Add a new column 'Symbol' with a progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Counter for managing requests\n",
    "request_counter = 0\n",
    "\n",
    "def wrapper_function(ncbi_gene_id):\n",
    "    global request_counter\n",
    "    request_counter += 1\n",
    "\n",
    "    # Every 3 requests, pause for 1 second\n",
    "    if request_counter % 3 == 0:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return get_gene_symbol(ncbi_gene_id)\n",
    "\n",
    "# Apply the function with tqdm\n",
    "df['Gene Symbol'] = df['Gene'].progress_apply(wrapper_function)\n",
    "\n",
    "# Extract unique MeSH IDs\n",
    "mesh_ids = df['Disease'].unique()\n",
    "\n",
    "def get_disease_name_from_mesh(mesh_id):\n",
    "    if mesh_id == 'D002493':\n",
    "        return 'Unspecified'\n",
    "    \n",
    "    url = f\"https://id.nlm.nih.gov/mesh/{mesh_id}.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if the data is a dictionary or a list\n",
    "        if isinstance(data, dict):\n",
    "            # Extract the disease name from the 'label' field if it's a dictionary\n",
    "            disease_name = data.get('label', {}).get('@value', None)\n",
    "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "            # If the data is a list of dictionaries, extract the name from the first dictionary\n",
    "            disease_name = data[0].get('label', {}).get('@value', None)\n",
    "        else:\n",
    "            # If data format is unexpected\n",
    "            disease_name = None\n",
    "        \n",
    "        return disease_name\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data for MeSH ID {mesh_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize dictionary and list to store MeSH ID to Disease Name mapping and failed IDs\n",
    "mesh_to_disease_dict = {}\n",
    "failed_mesh_ids = []\n",
    "\n",
    "# Fetch disease names for each MeSH ID\n",
    "for i, mesh_id in enumerate(tqdm(mesh_ids, desc=\"Fetching Disease Names\")):\n",
    "    # Fetch the disease name\n",
    "    disease_name = get_disease_name_from_mesh(mesh_id)\n",
    "    if disease_name:\n",
    "        mesh_to_disease_dict[mesh_id] = disease_name\n",
    "    else:\n",
    "        failed_mesh_ids.append(mesh_id)\n",
    "    \n",
    "    # Sleep for 1 second after every 3 iterations\n",
    "    if (i + 1) % 3 == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Add 'Disease' column to DataFrame\n",
    "df['Disease Symbol'] = df['Disease'].map(mesh_to_disease_dict)\n",
    "\n",
    "# Print failed MeSH IDs\n",
    "if failed_mesh_ids:\n",
    "    print(\"Failed to fetch data for the following MeSH IDs:\")\n",
    "    print(failed_mesh_ids)\n",
    "\n",
    "df['Disease Symbol'] = df['Disease Symbol'].fillna('ASPARTYL-tRNA SYNTHETASE DEFICIENCY')\n",
    "df['Disease Symbol'] = df['Disease Symbol'].replace('Unspecified', 'CNS DIS')\n",
    "df['gd_pair'] = df['Gene Symbol'].astype(str) + '-' + df['Disease Symbol']\n",
    "\n",
    "# Group by 'gd_pair' which refers to gene-disease pairs\n",
    "grouped = df.groupby('gd_pair')\n",
    "\n",
    "# Calculate the total Causal_annotations for each gene-disease pair\n",
    "causal = grouped['Causal_annotations'].sum()\n",
    "\n",
    "# Count the number of gene-disease abstracts for each gene-disease pair (assuming 'gd abstract count' column represents this)\n",
    "gd_abstract_count = grouped['gd abstract count'].sum()\n",
    "\n",
    "# Create a DataFrame for easier filtering and plotting\n",
    "stats_df = pd.DataFrame({\n",
    "    'gd_abstract_count': gd_abstract_count,\n",
    "    'causal': causal,\n",
    "    'gd_pair': grouped['gd_pair'].first()\n",
    "})\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(stats_df['gd_abstract_count'], stats_df['causal'], color='b', alpha=0.6)\n",
    "\n",
    "# Add labels to points where Causal_annotations > 5\n",
    "for i, row in stats_df.iterrows():\n",
    "    if row['causal'] > 5:\n",
    "        plt.text(row['gd_abstract_count'], row['causal'], row['gd_pair'], fontsize=11, ha='right')\n",
    "\n",
    "# Set x-axis to log scale\n",
    "plt.xscale('log')\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel('# Gene-Disease Abstract Count', fontsize=19)\n",
    "plt.ylabel('# Causal Annotations', fontsize=19)\n",
    "\n",
    "# Add grid and save the plot\n",
    "plt.grid(True)\n",
    "plt.savefig('gd_causal_log.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ae635-e8e0-4a92-abb1-bbc3084eafe3",
   "metadata": {},
   "source": [
    "Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c89b47-fe2b-4dc8-b331-e73834a80021",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = df.sort_values(by=['Causal/Total','Causal_annotations'], ascending=False)[:20]\n",
    "top_4 = top_20[['Gene Symbol', 'Disease Symbol', 'Causal/Total']][:4]\n",
    "top_4.rename(columns={'Gene Symbol':'Gene','Disease Symbol':'Disease','Causal/Total':'Causal Score'}, inplace=True)\n",
    "top_4.to_csv('causal ratio top 4.csv',index=False)\n",
    "top_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441b95e-1404-4b58-9fb2-7e08a10f4248",
   "metadata": {},
   "source": [
    "Fig. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd66a72-d5d7-4068-a08d-9fdb9db32e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 25 graph\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(40)  # You can choose any number as the seed\n",
    "\n",
    "# Sort the DataFrame by 'Causal/Total' and 'Causal_annotations' in descending order and keep the top 250\n",
    "top_df = df.sort_values(by=['Causal/Total', 'Causal_annotations'], ascending=False).head(20)\n",
    "\n",
    "# Filter out the genes to remove\n",
    "top_df_filtered = top_df\n",
    "\n",
    "# Create a graph\n",
    "g = ig.Graph()\n",
    "\n",
    "# Add vertices for Genes (Gene Symbol) and Diseases (Disease Symbol)\n",
    "genes = top_df_filtered['Gene Symbol'].unique()\n",
    "diseases = top_df_filtered['Disease Symbol'].unique()\n",
    "g.add_vertices(list(genes) + list(diseases))\n",
    "\n",
    "# Add edges with weights (Causal/Total)\n",
    "for _, row in top_df_filtered.iterrows():\n",
    "    g.add_edge(row['Gene Symbol'], row['Disease Symbol'], weight=row['Causal/Total'])\n",
    "\n",
    "# Normalize the edge weights to range from 0 to 1\n",
    "weights = [e[\"weight\"] for e in g.es]\n",
    "norm = mcolors.Normalize(vmin=0.5, vmax=1.0)\n",
    "\n",
    "# Create a custom colormap that transitions from light green to dark green\n",
    "def create_custom_green_cmap(start_color='lightgreen', end_color='darkgreen'):\n",
    "    colors = [start_color, end_color]\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_green', colors)\n",
    "    return cmap\n",
    "\n",
    "green_cmap = create_custom_green_cmap()\n",
    "\n",
    "# Apply the custom colormap to the normalized weights\n",
    "edge_colors = [green_cmap(norm(weight)) for weight in weights]\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "layout = g.layout(\"fr\")  # The layout is now reproducible due to the fixed seed\n",
    "\n",
    "# Plot the main graph\n",
    "for idx, vertex in enumerate(g.vs[\"name\"]):\n",
    "    x, y = layout[idx]\n",
    "    if vertex in genes:\n",
    "        circle = plt.Circle((x, y), 0.4, color='skyblue', zorder=3)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, vertex, horizontalalignment='center', verticalalignment='center', color=\"black\", fontsize=12, zorder=4)\n",
    "    elif vertex in diseases:\n",
    "        rect_width = max(0.22 * len(vertex), 0.5)  # Adjust width based on text length, with a minimum width\n",
    "        rect_height = 0.35  # Adjust height based on your layout scale\n",
    "        rect = mpatches.Rectangle(\n",
    "            (x - rect_width / 2, y - rect_height / 2), \n",
    "            rect_width, \n",
    "            rect_height, \n",
    "            facecolor=\"white\", edgecolor=\"black\", linewidth=2, zorder=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, vertex, horizontalalignment='center', verticalalignment='center', color=\"black\", fontsize=12, zorder=3)\n",
    "\n",
    "# Plot edges with varying widths and colors\n",
    "for e in g.es:\n",
    "    source, target = e.tuple\n",
    "    x_coords = [layout[source][0], layout[target][0]]\n",
    "    y_coords = [layout[source][1], layout[target][1]]\n",
    "    ax.plot(x_coords, y_coords, color=edge_colors[e.index], lw=2 + 8 * norm(e['weight']), zorder=1)\n",
    "\n",
    "# Add color bar for edge weights\n",
    "sm = plt.cm.ScalarMappable(cmap=green_cmap, norm=norm)\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Causal Score', fontsize=16)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "# cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0]) \n",
    "# cbar.ax.set_ylim([0, 1.0])\n",
    "\n",
    "# Add legend to the main plot with increased fontsize\n",
    "ax.legend(handles=[\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Gene', markerfacecolor='skyblue', markersize=10),\n",
    "    mpatches.Rectangle((0, 0), 1, 1, facecolor='white', edgecolor='black', linewidth=2, label='Disease')\n",
    "], loc='upper right', fontsize=14)  # Increase fontsize to 14\n",
    "\n",
    "ax.set_xticks([])  # Remove x-axis ticks\n",
    "ax.set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the graph as an image file\n",
    "fig.savefig('cred_causal_ratio_with_inset_20.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5aa5f-8c92-49f0-85bb-740d7b2bd97b",
   "metadata": {},
   "source": [
    "Fig. 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962027e-ada2-4f4f-9521-5b43d8bc4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 250 graph\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(60)  # You can choose any number as the seed\n",
    "\n",
    "# List of genes to remove\n",
    "genes_to_remove = [\n",
    "    'TREM2','TYROBP','ATF4','HBB','BRCA2','UBESA','Ubesa','DRD4',\n",
    "    'LMNA','Lmna','EMD','Emd','FHL1','TMEM43','HEXA','HEXB','NF2',\n",
    "    'ABCD1','Abcd1','NOTCH2','Galc','SLC40A1','Hamp','HFE','HAMP',\n",
    "    'TFR2','HJV','GNE','Galc','AR','MSR1','CFTR','HTT','Htt','TCAP',\n",
    "    'DMD','BRCA1','PQBP1','SHOX','UBE3A','Ube3a'\n",
    "]\n",
    "\n",
    "# Sort the DataFrame by 'Causal/Total' and 'Causal_annotations' in descending order and keep the top 250\n",
    "top_df = df.sort_values(by=['Causal/Total', 'Causal_annotations'], ascending=False).head(250)\n",
    "\n",
    "# Filter out the genes to remove\n",
    "top_df_filtered = top_df[~top_df['Gene Symbol'].isin(genes_to_remove)]\n",
    "\n",
    "# Create a graph\n",
    "g = ig.Graph()\n",
    "\n",
    "# Add vertices for Genes (Gene Symbol) and Diseases (Disease Symbol)\n",
    "genes = top_df_filtered['Gene Symbol'].unique()\n",
    "diseases = top_df_filtered['Disease Symbol'].unique()\n",
    "g.add_vertices(list(genes) + list(diseases))\n",
    "\n",
    "# Add edges with weights (Causal/Total)\n",
    "for _, row in top_df_filtered.iterrows():\n",
    "    g.add_edge(row['Gene Symbol'], row['Disease Symbol'], weight=row['Causal/Total'])\n",
    "\n",
    "# Normalize the edge weights to range from 0 to 1\n",
    "weights = [e[\"weight\"] for e in g.es]\n",
    "norm = mcolors.Normalize(vmin=min(weights), vmax=max(weights))\n",
    "\n",
    "# Create a custom colormap that transitions from light green to dark green\n",
    "def create_custom_green_cmap(start_color='lightgreen', end_color='darkgreen'):\n",
    "    colors = [start_color, end_color]\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_green', colors)\n",
    "    return cmap\n",
    "\n",
    "green_cmap = create_custom_green_cmap()\n",
    "\n",
    "# Apply the custom colormap to the normalized weights\n",
    "edge_colors = [green_cmap(norm(weight)) for weight in weights]\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(25, 20))\n",
    "layout = g.layout(\"fr\")  # The layout is now reproducible due to the fixed seed\n",
    "\n",
    "# Plot the main graph\n",
    "for idx, vertex in enumerate(g.vs[\"name\"]):\n",
    "    x, y = layout[idx]\n",
    "    if vertex in genes:\n",
    "        circle = plt.Circle((x, y), 0.4, color='skyblue', zorder=3)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, vertex, horizontalalignment='center', verticalalignment='center', color=\"black\", fontsize=12, zorder=4)\n",
    "    elif vertex in diseases:\n",
    "        rect_width = max(0.23 * len(vertex), 0.5)  # Adjust width based on text length, with a minimum width\n",
    "        rect_height = 0.45  # Adjust height based on your layout scale\n",
    "        rect = mpatches.Rectangle(\n",
    "            (x - rect_width / 2, y - rect_height / 2), \n",
    "            rect_width, \n",
    "            rect_height, \n",
    "            facecolor=\"white\", edgecolor=\"black\", linewidth=2, zorder=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, vertex, horizontalalignment='center', verticalalignment='center', color=\"black\", fontsize=12, zorder=3)\n",
    "\n",
    "# Plot edges with varying widths and colors\n",
    "for e in g.es:\n",
    "    source, target = e.tuple\n",
    "    x_coords = [layout[source][0], layout[target][0]]\n",
    "    y_coords = [layout[source][1], layout[target][1]]\n",
    "    ax.plot(x_coords, y_coords, color=edge_colors[e.index], lw=2 + 8 * norm(e['weight']), zorder=1)\n",
    "\n",
    "# Add inset axes (zoomed-in region)\n",
    "inset_ax = inset_axes(ax, width=\"35%\", height=\"28%\", loc=\"lower right\", borderpad=2)\n",
    "\n",
    "# Define the region to zoom in on (adjust to your needs)\n",
    "inset_xlim = (-8, 2)  # Adjust to the region you want to zoom in on\n",
    "inset_ylim = (4, 10.2)  # Adjust to the region you want to zoom in on\n",
    "\n",
    "inset_ax.set_xlim(inset_xlim)\n",
    "inset_ax.set_ylim(inset_ylim)\n",
    "\n",
    "# Replot the same graph inside the inset\n",
    "for idx, vertex in enumerate(g.vs[\"name\"]):\n",
    "    x, y = layout[idx]\n",
    "    if inset_xlim[0] <= x <= inset_xlim[1] and inset_ylim[0] <= y <= inset_ylim[1]:\n",
    "        if vertex in genes:\n",
    "            circle = plt.Circle((x, y), 0.3, color='skyblue', zorder=3)\n",
    "            inset_ax.add_patch(circle)\n",
    "            inset_ax.text(x, y, vertex, horizontalalignment='center', verticalalignment='center', color=\"black\", fontsize=8, zorder=4)\n",
    "        elif vertex in diseases:\n",
    "            rect_width = max(0.15 * len(vertex), 0.5)  # Adjust width based on text length, with a minimum width\n",
    "            rect_height = 0.35  # Adjust height based on your layout scale\n",
    "            rect = mpatches.Rectangle(\n",
    "                (x - rect_width / 2, y - rect_height / 2), \n",
    "                rect_width, \n",
    "                rect_height, \n",
    "                facecolor=\"white\", edgecolor=\"black\", linewidth=2, zorder=2\n",
    "            )\n",
    "            inset_ax.add_patch(rect)\n",
    "            inset_ax.text(x, y, vertex, horizontalalignment='center', verticalalignment='center', color=\"black\", fontsize=8, zorder=3)\n",
    "\n",
    "# Plot edges inside the inset\n",
    "for e in g.es:\n",
    "    source, target = e.tuple\n",
    "    x_coords = [layout[source][0], layout[target][0]]\n",
    "    y_coords = [layout[source][1], layout[target][1]]\n",
    "    if (inset_xlim[0] <= x_coords[0] <= inset_xlim[1] and inset_ylim[0] <= y_coords[0] <= inset_ylim[1]) or \\\n",
    "       (inset_xlim[0] <= x_coords[1] <= inset_xlim[1] and inset_ylim[0] <= y_coords[1] <= inset_ylim[1]):\n",
    "        inset_ax.plot(x_coords, y_coords, color=edge_colors[e.index], lw=2 + 8 * norm(e['weight']), zorder=1)\n",
    "\n",
    "# Add color bar for edge weights\n",
    "sm = plt.cm.ScalarMappable(cmap=green_cmap, norm=norm)\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Causal Score', fontsize=18)\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "# Add legend to the main plot with increased fontsize\n",
    "ax.legend(handles=[\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label='Gene', markerfacecolor='skyblue', markersize=10),\n",
    "    mpatches.Rectangle((0, 0), 1, 1, facecolor='white', edgecolor='black', linewidth=2, label='Disease')\n",
    "], loc='upper right', fontsize=20)  # Increase fontsize to 14\n",
    "\n",
    "ax.set_xticks([])  # Remove x-axis ticks\n",
    "ax.set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "inset_ax.set_xticks([])  # Remove x-axis ticks\n",
    "inset_ax.set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the graph as an image file\n",
    "fig.savefig('cred_causal_ratio_with_inset.png', bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
