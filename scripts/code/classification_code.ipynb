{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ff07c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioBERT model loaded\n",
      "Tokenizer loaded\n",
      "Dataset created\n",
      "Embeddings generated\n",
      "Doing classification using SVM...\n",
      "Results of SVM classifier are: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       561\n",
      "           1       0.61      0.35      0.45        54\n",
      "\n",
      "    accuracy                           0.92       615\n",
      "   macro avg       0.78      0.67      0.70       615\n",
      "weighted avg       0.91      0.92      0.91       615\n",
      "\n",
      "Doing classification using XG Boost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/home/ubuntu/miniconda3/envs/hugging/lib/python3.10/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of XG Boost are: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       561\n",
      "           1       0.34      0.37      0.35        54\n",
      "\n",
      "    accuracy                           0.88       615\n",
      "   macro avg       0.64      0.65      0.64       615\n",
      "weighted avg       0.89      0.88      0.88       615\n",
      "\n",
      "Doing classification using Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:    7.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Random Forest are: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       561\n",
      "           1       0.37      0.31      0.34        54\n",
      "\n",
      "    accuracy                           0.89       615\n",
      "   macro avg       0.65      0.63      0.64       615\n",
      "weighted avg       0.89      0.89      0.89       615\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 200 out of 200 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=5)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from interpretation_code import interpretation\n",
    "# from k_fold_cv import k_fold\n",
    "\n",
    "\n",
    "\n",
    "class model_trainer:\n",
    "\n",
    "    def __init__(self, train_path, test_path, model_name):\n",
    "        self.train_df=self.load_dataset(train_path)\n",
    "        self.test_df=self.load_dataset(test_path)\n",
    "        self.model_name=self.load_model(model_name)\n",
    "        self.tokenizer=self.load_tokenizer(model_name)\n",
    "#         self.model_name.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "\n",
    "    def load_model(self,model_name):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        print(\"BioBERT model loaded\")\n",
    "        return biobert\n",
    "\n",
    "    def load_tokenizer(self,model_name):\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#         tokenizer.add_tokens([\"GeneSrc\", \"DiseaseTgt\", \"causative\", \"causal\", \"cause\", \"causing\", \"caused\"])\n",
    "        print(\"Tokenizer loaded\")\n",
    "        return tokenizer\n",
    "\n",
    "    def load_dataset(self,data_path):\n",
    "        df = pd.read_csv(data_path, delimiter='\\t')\n",
    "        return df\n",
    "    \n",
    "    def remove_stopwords(self,text):\n",
    "        #nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    \n",
    "    def create_dataset(self):\n",
    "        # Apply the remove_stopwords function to the 'sentence' column\n",
    "        self.train_df['sentence'] = self.train_df['sentence'].apply(self.remove_stopwords)\n",
    "        self.test_df['sentence'] = self.test_df['sentence'].apply(self.remove_stopwords)\n",
    "\n",
    "        X_train = self.train_df['sentence'].tolist()\n",
    "        y_train = self.train_df['label'].tolist()\n",
    "\n",
    "        X_test = self.test_df['sentence'].tolist()\n",
    "        y_test = self.test_df['label'].tolist()\n",
    "        \n",
    "        print(\"Dataset created\")\n",
    "        \n",
    "        return X_train,y_train,X_test,y_test\n",
    "    \n",
    "    def get_specific_token_embeddings(self,sentence):\n",
    "        # 1. Tokenize the input sentence\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "\n",
    "        # 2. Find the indices of \"@GeneSrc\" and \"@DiseaseTgt$\"\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        gene_src_token = self.tokenizer.tokenize(\"@GeneSrc$\")\n",
    "        disease_tgt_token = self.tokenizer.tokenize(\"@DiseaseTgt$\")\n",
    "\n",
    "        gene_src_indices = [i for i, token in enumerate(tokenized_sentence) if token in gene_src_token]\n",
    "        disease_tgt_indices = [i for i, token in enumerate(tokenized_sentence) if token in disease_tgt_token]\n",
    "\n",
    "        # Run the sentence through BioBERT\n",
    "        with torch.no_grad():\n",
    "            #print(self.model_name)\n",
    "            outputs = self.model_name(**inputs)\n",
    "        embeddings = outputs['last_hidden_state'][0]  # Extracting embeddings for the whole sentence\n",
    "\n",
    "        # 3. Retrieve the embeddings for the surrounding tokens\n",
    "        context_range = 2\n",
    "\n",
    "        def get_context_embeddings(indices):\n",
    "            context_embeddings = []\n",
    "            for idx in indices:\n",
    "                start = max(0, idx - context_range)\n",
    "                end = min(idx + context_range + 1, len(tokenized_sentence))\n",
    "                context = embeddings[start:end]\n",
    "                context_embeddings.append(context)\n",
    "            return torch.cat(context_embeddings).view(-1, 768)\n",
    "\n",
    "        gene_src_embeddings = get_context_embeddings(gene_src_indices)\n",
    "        disease_tgt_embeddings = get_context_embeddings(disease_tgt_indices)\n",
    "\n",
    "        # 4. Compute the average of the embeddings\n",
    "        avg_gene_src_embedding = torch.mean(gene_src_embeddings, dim=0)\n",
    "        avg_disease_tgt_embedding = torch.mean(disease_tgt_embeddings, dim=0)\n",
    "\n",
    "        combined_embedding = torch.cat([avg_gene_src_embedding, avg_disease_tgt_embedding], dim=0)\n",
    "        combined_embedding_np = combined_embedding.cpu().numpy().reshape(1, -1)  # Convert tensor to NumPy array and reshape to 2D\n",
    "\n",
    "        if np.isnan(combined_embedding_np).any():\n",
    "            print(sentence)\n",
    "\n",
    "        return combined_embedding_np\n",
    "\n",
    "        \n",
    "        \n",
    "    def generate_embeddings(self):\n",
    "        X_train,y_train,X_test,y_test=self.create_dataset()\n",
    "        try:\n",
    "            X_train_embeddings = np.vstack([self.get_specific_token_embeddings(sentence) for sentence in tqdm(X_train)])\n",
    "            X_test_embeddings = np.vstack([self.get_specific_token_embeddings(sentence) for sentence in tqdm(X_test)])\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        print(\"Embeddings generated\")\n",
    "        return X_train_embeddings,X_test_embeddings\n",
    "    \n",
    "    def drop_null_embeddings(self,X_train_embeddings,X_test_embeddings,y_train,y_test):\n",
    "        X_train_embeddings=pd.DataFrame(X_train_embeddings).dropna()\n",
    "        X_test_embeddings=pd.DataFrame(X_test_embeddings).dropna()\n",
    "\n",
    "        test_ind=[i for i in range(0,len(X_test_embeddings)) if i not in X_test_embeddings.index]\n",
    "        train_ind=[i for i in range(0,len(X_train_embeddings)) if i not in X_train_embeddings.index]\n",
    "        for i in train_ind:\n",
    "            y_train.pop(i)\n",
    "        for i in test_ind:\n",
    "            y_test.pop(i)\n",
    "            \n",
    "        return X_train_embeddings,X_test_embeddings,y_train,y_test\n",
    "    \n",
    "    def svm_classifiation(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using SVM...\")\n",
    "        clf = SVC(kernel='poly', degree=10, probability=True, C=10, class_weight={0:1, 1:30})\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of SVM classifier are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "    def xg_boost_classification(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using XG Boost...\")\n",
    "        \n",
    "        clf = xgb.XGBClassifier(scale_pos_weight=500, max_depth=40, learning_rate=0.1, n_estimators=400, gamma=0.1)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Get probabilities\n",
    "        y_prob = clf.predict_proba(X_test_embeddings)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        \n",
    "        # Save the model to a file\n",
    "        dump(clf, 'CDR_trained_model_xgb.joblib') \n",
    "\n",
    "        print(\"Results of XG Boost are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    def random_forest(self, X_train_embeddings,y_train,X_test_embeddings,y_test):\n",
    "        print(\"Doing classification using Random Forest...\")\n",
    "        \n",
    "\n",
    "        # Instantiate the Random Forest Classifier\n",
    "        clf = RandomForestClassifier(n_estimators=200, max_depth=50, max_leaf_nodes= 500, n_jobs= 5, max_features=\"sqrt\", class_weight={0:1, 1:30}, verbose=True)\n",
    "        clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "        # Predict and Evaluate\n",
    "        y_pred = clf.predict(X_test_embeddings)\n",
    "        print(\"Results of Random Forest are: \")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "\n",
    "        \n",
    "    def interpretation_call(self):\n",
    "\n",
    "        causal_test_df=self.test_df[self.test_df[\"label\"]==1]\n",
    "        causal_df_unique = causal_test_df.drop_duplicates(subset=['index', 'id1', 'id2'])\n",
    "\n",
    "        samp_abst=causal_df_unique[causal_df_unique[\"index\"]==25064704]\n",
    "\n",
    "        # Load the model from a file\n",
    "        clf = load('/home/ubuntu/CRED_application/CRED_trained_model_new_data.joblib')\n",
    "\n",
    "        model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "        biobert = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        for _, row in tqdm(causal_df_unique.iterrows()):\n",
    "            ranked, word_importance = interpretation(row, clf, tokenizer, biobert, self.get_specific_token_embeddings)\n",
    "\n",
    "    \n",
    "    \n",
    "def main():       \n",
    "    mod_tr=model_trainer('new_train_data', 'test_data', \"dmis-lab/biobert-base-cased-v1.1\")\n",
    "    \n",
    "    #To generate embeddings uncomment the below line\n",
    "#     X_train_embeddings,X_test_embeddings= mod_tr.generate_embeddings()\n",
    "    \n",
    "#     Reading the training embeddings from the text file\n",
    "    X_train,y_train,X_test,y_test = mod_tr.create_dataset()\n",
    "#     X_train_embeddings = pd.read_csv('/home/ubuntu/CRED_application/X_train_embeddings_new_data.txt', sep='\\t', header=None)\n",
    "#     X_test_embeddings = pd.read_csv('/home/ubuntu/CRED_application/X_test_embeddings.txt', sep='\\t', header=None)\n",
    "    X_train_embeddings = pd.read_csv('X_train_embeddings_new_data.txt', sep='\\t', header=None)\n",
    "    X_test_embeddings = pd.read_csv('X_test_embeddings.txt', sep='\\t', header=None)\n",
    "\n",
    "    X_train_embeddings,X_test_embeddings,y_train,y_test = mod_tr.drop_null_embeddings(X_train_embeddings,X_test_embeddings,y_train,y_test)\n",
    "    print(\"Embeddings generated\")\n",
    "    \n",
    "    mod_tr.svm_classifiation(X_train_embeddings,y_train,X_test_embeddings,y_test)\n",
    "    mod_tr.xg_boost_classification(X_train_embeddings,y_train,X_test_embeddings,y_test)\n",
    "    mod_tr.random_forest(X_train_embeddings,y_train,X_test_embeddings,y_test)\n",
    "    \n",
    "    #For Interpretaion uncomment the below 2 lines\n",
    "#     print(\"Interpretaion started. It may take upto 10 minutes...\")\n",
    "#     mod_tr.interpretation_call()\n",
    "    \n",
    "    #For 4-fold cross validation uncomment the below 2 lines\n",
    "#     print(\"4-fold cross validation started. It may take few minutes...\")\n",
    "#     k_fold(mod_tr.train_df, mod_tr.test_df, mod_tr.get_specific_token_embeddings)\n",
    "\n",
    "\n",
    "#     # Saving the training embeddings to a text file\n",
    "#     pd.DataFrame(X_train_embeddings).to_csv('X_train_embeddings_without_extra_tokens.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "#     # Saving the test embeddings to a text file\n",
    "#     pd.DataFrame(X_test_embeddings).to_csv('val_embeddings_without_extra_tokens.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "              \n",
    "    \n",
    "\n",
    "    \n",
    "main()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c9919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2667d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
