{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "# clf = load('../CRED_application/Best_CRED_trained_model_new_data_svm.joblib')\n",
    "# clf = load('CDR_trained_model_xgb.joblib')\n",
    "clf = load('../CRED_application/CRED_trained_model_new_data_xgb.joblib')\n",
    "\n",
    "# clf = load('CDR_trained_model.joblib')\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained( \"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model = BertModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "biobert=model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "        #nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_token_embeddings(sentence):\n",
    "        # 1. Tokenize the input sentence\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        # 2. Find the indices of \"@GeneSrc\" and \"@DiseaseTgt$\"\n",
    "        tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "        gene_src_token = tokenizer.tokenize(\"@GeneSrc$\")\n",
    "        disease_tgt_token = tokenizer.tokenize(\"@DiseaseTgt$\")\n",
    "\n",
    "        gene_src_indices = [i for i, token in enumerate(tokenized_sentence) if token in gene_src_token]\n",
    "        disease_tgt_indices = [i for i, token in enumerate(tokenized_sentence) if token in disease_tgt_token]\n",
    "\n",
    "        # Run the sentence through BioBERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs['last_hidden_state'][0]  # Extracting embeddings for the whole sentence\n",
    "\n",
    "        # 3. Retrieve the embeddings for the surrounding tokens\n",
    "        context_range = 2\n",
    "\n",
    "        def get_context_embeddings(indices):\n",
    "            context_embeddings = []\n",
    "            for idx in indices:\n",
    "                start = max(0, idx - context_range)\n",
    "                end = min(idx + context_range + 1, len(tokenized_sentence))\n",
    "                context = embeddings[start:end]\n",
    "                context_embeddings.append(context)\n",
    "            return torch.cat(context_embeddings).view(-1, 768)\n",
    "\n",
    "        gene_src_embeddings = get_context_embeddings(gene_src_indices)\n",
    "        disease_tgt_embeddings = get_context_embeddings(disease_tgt_indices)\n",
    "\n",
    "        # 4. Compute the average of the embeddings\n",
    "        avg_gene_src_embedding = torch.mean(gene_src_embeddings, dim=0)\n",
    "        avg_disease_tgt_embedding = torch.mean(disease_tgt_embeddings, dim=0)\n",
    "\n",
    "        combined_embedding = torch.cat([avg_gene_src_embedding, avg_disease_tgt_embedding], dim=0)\n",
    "        combined_embedding_np = combined_embedding.cpu().numpy().reshape(1, -1)  # Convert tensor to NumPy array and reshape to 2D\n",
    "\n",
    "        if np.isnan(combined_embedding_np).any():\n",
    "            print(sentence)\n",
    "\n",
    "        return combined_embedding_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [08:04,  3.67s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Code for perturbation and generating importance scores\n",
    "\n",
    "def perturb_sentence(sentence, tokenizer):\n",
    "    \"\"\"Perturb the input sentence by masking out one word at a time.\"\"\"\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    perturbed_sentences = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        perturbed = words[:i] + ['[MASK]'] + words[i+1:]\n",
    "        perturbed_sentences.append(tokenizer.convert_tokens_to_string(perturbed))\n",
    "    \n",
    "    return perturbed_sentences\n",
    "\n",
    "def compute_influences(sentence, clf, tokenizer, biobert):\n",
    "    \"\"\"Compute the influence of each word on the classifier's prediction.\"\"\"\n",
    "    original_confidences=[]\n",
    "    perturbed_confidences=[]\n",
    "    perturbed_sentences = perturb_sentence(sentence, tokenizer)\n",
    "    original_embedding = get_specific_token_embeddings(sentence).reshape(1, -1)\n",
    "    original_confidence = clf.predict_proba(original_embedding)[0][1]\n",
    "    original_confidences.append(original_confidence)\n",
    "    \n",
    "    # Add prediction of model for original sentence\n",
    "    original_prediction = clf.predict(original_embedding)[0]\n",
    "    \n",
    "\n",
    "    influences = []\n",
    "    org_probs = []\n",
    "    perturbed_probs = []\n",
    "    for perturbed in perturbed_sentences:\n",
    "        perturbed_embedding = get_specific_token_embeddings(perturbed).reshape(1, -1)\n",
    "        perturbed_confidence = clf.predict_proba(perturbed_embedding)[0][1]\n",
    "        perturbed_confidences.append(perturbed_confidence)\n",
    "        \n",
    "        influence=(max(perturbed_confidence-0.5, original_confidence-0.5, 0))*(original_confidence-perturbed_confidence)\n",
    "        #influence = abs(original_confidence - perturbed_confidence)\n",
    "        #influence =  perturbed_confidence - original_confidence\n",
    "        influences.append(influence)\n",
    "        org_probs.append(original_confidence)\n",
    "        perturbed_probs.append(perturbed_confidence)\n",
    "        \n",
    "\n",
    "    return influences, org_probs, perturbed_probs, original_prediction\n",
    "\n",
    "def rank_words_by_influence(sentence, influences, tokenizer, org_probs, perturbed_probs):\n",
    "    #print(sentence)\n",
    "    \"\"\"Rank words by their influence.\"\"\"\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    all_words = []\n",
    "    all_influence_scores = []\n",
    "    all_org_prob_scores = []\n",
    "    all_perturbed_prob_scores = []\n",
    "    importance = []\n",
    "    org_prob_importance = []\n",
    "    perturbed_prob_importance = []\n",
    "    word = \"\"\n",
    "    old_word = None\n",
    "    for influence, org_prob, perturbed_prob, token in zip(influences, org_probs, perturbed_probs, words):\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        if token.startswith(\"#\"):\n",
    "            new_token = token.replace('#', '')\n",
    "            word += new_token.strip()\n",
    "            importance.append(influence)\n",
    "            org_prob_importance.append(org_prob)\n",
    "            perturbed_prob_importance.append(perturbed_prob)\n",
    "        else:\n",
    "            if old_word is None:\n",
    "                all_words.append(word)\n",
    "                all_influence_scores.append(influence)\n",
    "                all_org_prob_scores.append(org_prob)\n",
    "                all_perturbed_prob_scores.append(perturbed_prob)\n",
    "            else:\n",
    "                if old_word.startswith(\"#\"):\n",
    "                    all_words.append(word)\n",
    "                    word = \"\" + token\n",
    "                    influence_score = sum(importance) / len(importance)\n",
    "                    org_prob_score = sum(org_prob_importance) / len(org_prob_importance)\n",
    "                    perturbed_prob_score = sum(perturbed_prob_importance) / len(perturbed_prob_importance)\n",
    "                    all_influence_scores.append(influence_score)\n",
    "                    all_org_prob_scores.append(org_prob_score)\n",
    "                    all_perturbed_prob_scores.append(perturbed_prob_score)\n",
    "                else:\n",
    "                    all_words.append(old_word)\n",
    "                    word = \"\" + token\n",
    "                    importance.append(influence)\n",
    "                    org_prob_importance.append(org_prob)\n",
    "                    perturbed_prob_importance.append(perturbed_prob)\n",
    "                    all_influence_scores.append(old_imp)\n",
    "                    all_org_prob_scores.append(old_org_prob)\n",
    "                    all_perturbed_prob_scores.append(old_perturbed_prob)\n",
    "            old_word = token\n",
    "            old_imp = influence\n",
    "            old_org_prob = org_prob\n",
    "            old_perturbed_prob = perturbed_prob\n",
    "    \n",
    "    ranked_words = [word for _, word in sorted(zip(all_influence_scores, all_words), reverse=True)]\n",
    "    return ranked_words, all_words, all_influence_scores, all_org_prob_scores, all_perturbed_prob_scores\n",
    "\n",
    "\n",
    "def aggregate_word_importance(words, imp_scores, org_prob_scores, perturbed_prob_scores):\n",
    "    \"\"\"Aggregate scores for words into a single dictionary.\"\"\"\n",
    "    word_importance = {}\n",
    "\n",
    "    for word, imp_score, org_prob_score, perturbed_prob_score in zip(words, imp_scores, org_prob_scores, perturbed_prob_scores):\n",
    "        if len(word)>2:\n",
    "            if word in word_importance:\n",
    "                # Update existing entry with max scores\n",
    "                word_importance[word] = [max(word_importance[word][0], imp_score),\n",
    "                                         max(word_importance[word][1], org_prob_score),\n",
    "                                         max(word_importance[word][2], perturbed_prob_score)]\n",
    "            else:\n",
    "                # Create new entry for the word\n",
    "                word_importance[word] = [imp_score, org_prob_score, perturbed_prob_score]\n",
    "\n",
    "    return word_importance\n",
    "\n",
    "\n",
    "def save_word_importance_to_file(word_importance, row, original_prediction, filename='word_importance_score_cred_withpred_traindata_xgb.tsv'):\n",
    "    \"\"\"Save word importance scores to a file.\"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        if file.tell() == 0:  # Check if file is empty\n",
    "            file.write(\"PMID\\tgeneid\\tdiseaseid\\tword\\timportance_score\\torg_prob_score\\tperturbed_prob_score\\toriginal_prediction\\n\")\n",
    "        for word, scores in word_importance.items():\n",
    "            # Write index, id1, id2, word, importance score, org_prob_score, perturbed_prob_score, and original prediction\n",
    "            file.write(f\"{row['index']}\\t{row['id1']}\\t{row['id2']}\\t{word}\\t{scores[0]}\\t{scores[1]}\\t{scores[2]}\\t{original_prediction}\\n\")\n",
    "\n",
    "\n",
    "def main(row):\n",
    "    sentence = row['sentence']\n",
    "    influences, org_probs, perturbed_probs, original_prediction = compute_influences(sentence, clf, tokenizer, biobert)\n",
    "    \n",
    "    ranked, words, imp_scores, org_prob_scores, perturbed_prob_scores = rank_words_by_influence(sentence, influences, tokenizer, org_probs, perturbed_probs)\n",
    "    ranked = [i for i in ranked if len(i) > 1]\n",
    "    #print(\"Words ranked by influence:\", ranked)\n",
    "    word_importance = aggregate_word_importance(words, imp_scores, org_prob_scores, perturbed_prob_scores)\n",
    "    save_word_importance_to_file(word_importance, row, original_prediction)\n",
    "    return ranked, word_importance\n",
    "\n",
    "\n",
    "# Apply the main function for each value in the \"sentence\" column\n",
    "train_df = pd.read_csv('new_train_data', delimiter='\\t')\n",
    "train_df['sentence'] = train_df['sentence'].apply(remove_stopwords)\n",
    "causal_train_df=train_df[train_df[\"label\"]==1]\n",
    "causal_train_df_unique = causal_train_df.drop_duplicates(subset=['index', 'id1', 'id2'])\n",
    "\n",
    "\n",
    "# causal_test_df=test_df[test_df[\"label\"]==1]\n",
    "# causal_df_unique = causal_test_df.drop_duplicates(subset=['index', 'id1', 'id2'])\n",
    "\n",
    "# samp_abst=causal_df_unique[causal_df_unique[\"index\"]==25064704]\n",
    "\n",
    "for _, row in tqdm(causal_train_df_unique.iterrows()):\n",
    "    ranked, word_importance = main(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
